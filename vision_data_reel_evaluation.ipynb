{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yz_8FnGXl5g",
        "outputId": "1eaeaa3e-5a13-45b5-8dbb-7d77a5d29c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üíæ MONTAGE GOOGLE DRIVE\n",
            "======================================================================\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mont√©\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# CELLULE 1 : MONTAGE GOOGLE DRIVE\n",
        "# ==========================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üíæ MONTAGE GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mont√©\")\n",
        "else:\n",
        "    print(\"‚úÖ Google Drive d√©j√† mont√©\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pskMwCGcXqqP",
        "outputId": "52aa3bc5-9b7a-491a-fdda-d53ca867b3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üîß INSTALLATION DE L'ENVIRONNEMENT BARF\n",
            "======================================================================\n",
            "\n",
            "üì• Clonage du d√©p√¥t BARF...\n",
            "Cloning into 'bundle-adjusting-NeRF'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 102 (delta 35), reused 26 (delta 26), pack-reused 46 (from 1)\u001b[K\n",
            "Receiving objects: 100% (102/102), 191.17 KiB | 6.17 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "‚úÖ D√©p√¥t clon√©\n",
            "üìÇ R√©pertoire : /content/barf_workspace/bundle-adjusting-NeRF\n",
            "\n",
            "üì¶ Installation des d√©pendances...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "üîß Configuration module SSIM...\n",
            "‚úÖ Module SSIM cr√©√©\n",
            "\n",
            "üîß Patch de options.py...\n",
            "‚úÖ options.py patch√©\n",
            "\n",
            "üìù Configuration YAML...\n",
            "‚úÖ YAML configur√©\n",
            "\n",
            "======================================================================\n",
            "‚úÖ INSTALLATION TERMIN√âE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# CELLULE 2 : INSTALLATION BARF\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîß INSTALLATION DE L'ENVIRONNEMENT BARF\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Cr√©er workspace et cloner BARF\n",
        "WORKSPACE = '/content/barf_workspace'\n",
        "BARF_DIR = os.path.join(WORKSPACE, 'bundle-adjusting-NeRF')\n",
        "\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(BARF_DIR):\n",
        "    print(\"\\nüì• Clonage du d√©p√¥t BARF...\")\n",
        "    os.chdir(WORKSPACE)\n",
        "    !git clone https://github.com/chenhsuanlin/bundle-adjusting-NeRF.git\n",
        "    print(\"‚úÖ D√©p√¥t clon√©\")\n",
        "else:\n",
        "    print(\"‚úÖ D√©p√¥t BARF d√©j√† pr√©sent\")\n",
        "\n",
        "os.chdir(BARF_DIR)\n",
        "print(f\"üìÇ R√©pertoire : {os.getcwd()}\")\n",
        "\n",
        "# 2. Installer les d√©pendances\n",
        "print(\"\\nüì¶ Installation des d√©pendances...\")\n",
        "!pip install -q numpy matplotlib pillow tqdm tensorboard easydict pyyaml opencv-python lpips\n",
        "\n",
        "# 3. Cr√©er le module SSIM manquant\n",
        "print(\"\\nüîß Configuration module SSIM...\")\n",
        "os.makedirs('external', exist_ok=True)\n",
        "\n",
        "ssim_code = '''\"\"\"SSIM implementation\"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from math import exp\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def pytorch_ssim(img1, img2, window_size=11, size_average=True):\n",
        "    channel = img1.size()[1]\n",
        "    window = create_window(window_size, channel)\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "'''\n",
        "\n",
        "with open('external/pohsun_ssim.py', 'w') as f:\n",
        "    f.write(ssim_code)\n",
        "with open('external/__init__.py', 'w') as f:\n",
        "    f.write('')\n",
        "\n",
        "print(\"‚úÖ Module SSIM cr√©√©\")\n",
        "\n",
        "# 4. Patcher options.py pour √©viter les input() interactifs\n",
        "print(\"\\nüîß Patch de options.py...\")\n",
        "\n",
        "with open('options.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Supprimer les input() interactifs\n",
        "content = content.replace(\n",
        "    'override = input(\"override? (y/n) \")',\n",
        "    'override = \"y\"  # Auto-patched for Colab'\n",
        ")\n",
        "content = content.replace(\n",
        "    'retry = input(\"visdom port ({}) not open, retry? (y/n) \".format(opt.visdom.port))',\n",
        "    'retry = \"n\"  # Auto-patched for Colab'\n",
        ")\n",
        "\n",
        "with open('options.py', 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ options.py patch√©\")\n",
        "\n",
        "# 5. Modifier le fichier YAML pour barf_c2f\n",
        "print(\"\\nüìù Configuration YAML...\")\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_path = 'options/barf_blender.yaml'\n",
        "with open(yaml_path, 'r') as f:\n",
        "    config_yaml = yaml.safe_load(f)\n",
        "\n",
        "config_yaml['barf_c2f'] = [0.1, 0.5]\n",
        "config_yaml['visdom'] = False\n",
        "\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(config_yaml, f, default_flow_style=False)\n",
        "\n",
        "print(\"‚úÖ YAML configur√©\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ INSTALLATION TERMIN√âE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2vYuaA4YsiO",
        "outputId": "f2b4bf43-9c22-4d1e-8480-c25ec0d81b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üöÄ BARF - √âVALUATION DE TOUS LES CHECKPOINTS\n",
            "======================================================================\n",
            "\n",
            "üîß V√©rification SSIM...\n",
            "‚úÖ SSIM corrig√©e\n",
            "\n",
            "üîç V√©rification structure dataset...\n",
            "Dataset root: /content/drive/MyDrive/vision/images/vision_scene\n",
            "\n",
            "Fichiers transforms trouv√©s:\n",
            "  ‚Ä¢ transforms_train.json\n",
            "  ‚Ä¢ transforms_test.json\n",
            "  ‚Ä¢ transforms_val.json\n",
            "\n",
            "Premier chemin d'image trouv√©: ./images/img_76\n",
            "\n",
            "üîÑ Cr√©ation structure temporaire pour BARF...\n",
            "  ‚Ä¢ Copi√© transforms_train.json -> /tmp/barf_dataset/vision_scene/transforms_train.json\n",
            "  ‚Ä¢ Copi√© transforms_test.json -> /tmp/barf_dataset/vision_scene/transforms_test.json\n",
            "  ‚Ä¢ Copi√© transforms_val.json -> /tmp/barf_dataset/vision_scene/transforms_val.json\n",
            "  ‚Ä¢ Chemins corrig√©s dans transforms_train.json\n",
            "  ‚Ä¢ Chemins corrig√©s dans transforms_test.json\n",
            "  ‚Ä¢ Chemins corrig√©s dans transforms_val.json\n",
            "\n",
            "‚úÖ Structure temporaire cr√©√©e: /tmp/barf_dataset/vision_scene\n",
            "\n",
            "üìÇ 2 checkpoints trouv√©s dans /content/drive/MyDrive/barf_outputs/results/vision_20251207_2007/training/model:\n",
            "   ‚Ä¢ 78000.ckpt\n",
            "   ‚Ä¢ 80000.ckpt\n",
            "\n",
            "======================================================================\n",
            "üéØ D√âBUT DES √âVALUATIONS\n",
            "======================================================================\n",
            "\n",
            "[1/2] √âvaluation de 78000.ckpt...\n",
            "   Ex√©cution de la commande...\n",
            "   Return code: 0\n",
            "   ‚úÖ Commande ex√©cut√©e avec succ√®s\n",
            "   ‚úÖ PSNR: 23.88 dB | SSIM: 0.9900 | LPIPS: 0.6900\n",
            "\n",
            "[2/2] √âvaluation de 80000.ckpt...\n",
            "   Ex√©cution de la commande...\n",
            "   Return code: 0\n",
            "   ‚úÖ Commande ex√©cut√©e avec succ√®s\n",
            "   ‚úÖ PSNR: 23.92 dB | SSIM: 0.9900 | LPIPS: 0.6800\n",
            "\n",
            "======================================================================\n",
            "üìä R√âSUM√â GLOBAL\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Checkpoint                Iter     PSNR (dB)    SSIM       LPIPS     \n",
            "----------------------------------------------------------------------\n",
            "78000.ckpt                78000    23.88        0.9900     0.6900    \n",
            "80000.ckpt                80000    23.92        0.9900     0.6800    \n",
            "\n",
            "======================================================================\n",
            "üìà STATISTIQUES\n",
            "----------------------------------------------------------------------\n",
            "Checkpoints √©valu√©s : 2/2\n",
            "\n",
            "Meilleur PSNR       : 23.92 dB (80000.ckpt)\n",
            "Meilleur SSIM       : 0.9900 (78000.ckpt)\n",
            "Meilleur LPIPS      : 0.6800 (80000.ckpt)\n",
            "\n",
            "Moyenne PSNR        : 23.90 dB\n",
            "Moyenne SSIM        : 0.9900\n",
            "Moyenne LPIPS       : 0.6850\n",
            "\n",
            "üíæ CSV sauvegard√©: /content/drive/MyDrive/barf_outputs/results/evaluation/all_checkpoints_20251208_170719.csv\n",
            "\n",
            "======================================================================\n",
            "üìà √âVOLUTION DU PSNR\n",
            "----------------------------------------------------------------------\n",
            "78000.ckpt       23.88 dB\n",
            "80000.ckpt      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 23.92 dB\n",
            "\n",
            "üíæ Rapport d√©taill√©: /content/drive/MyDrive/barf_outputs/results/evaluation/rapport_complet_20251208_170719.txt\n",
            "üßπ Structure temporaire nettoy√©e: /tmp/barf_dataset\n",
            "\n",
            "======================================================================\n",
            "‚úÖ √âVALUATION COMPL√àTE TERMIN√âE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ BARF - √âVALUATION DE TOUS LES CHECKPOINTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "BARF_PATH = '/content/barf_workspace/bundle-adjusting-NeRF'\n",
        "nerf_path = os.path.join(BARF_PATH, 'model', 'nerf.py')\n",
        "\n",
        "# ==========================================\n",
        "# FIX SSIM (une seule fois)\n",
        "# ==========================================\n",
        "print(\"\\nüîß V√©rification SSIM...\")\n",
        "\n",
        "with open(nerf_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "ssim_fixed = False\n",
        "for i, line in enumerate(lines):\n",
        "    if 'ssim = pytorch_ssim' in line:\n",
        "        if 'unsqueeze' not in line:\n",
        "            indent = ' ' * (len(line) - len(line.lstrip()))\n",
        "            new_line = indent + 'ssim = pytorch_ssim(rgb_map.permute(0,3,1,2), var.image.permute(0,3,1,2)).item()\\n'\n",
        "            lines[i] = new_line\n",
        "            ssim_fixed = True\n",
        "\n",
        "if ssim_fixed:\n",
        "    with open(nerf_path, 'w') as f:\n",
        "        f.writelines(lines)\n",
        "    print(\"‚úÖ SSIM corrig√©e\")\n",
        "else:\n",
        "    print(\"‚úÖ SSIM d√©j√† correcte\")\n",
        "\n",
        "# ==========================================\n",
        "# CONFIG\n",
        "# ==========================================\n",
        "# Le chemin doit pointer vers le dossier contenant les transforms.json\n",
        "DATASET_ROOT = '/content/drive/MyDrive/vision/images/vision_scene'\n",
        "\n",
        "CHECKPOINTS_PATH = '/content/drive/MyDrive/barf_outputs/results/vision_20251207_2007/training/model'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/barf_outputs/results/evaluation'\n",
        "\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# V√âRIFIER LA STRUCTURE DU DATASET\n",
        "# ==========================================\n",
        "print(\"\\nüîç V√©rification structure dataset...\")\n",
        "print(f\"Dataset root: {DATASET_ROOT}\")\n",
        "\n",
        "# V√©rifier si les fichiers transforms existent\n",
        "transforms_files = [\n",
        "    os.path.join(DATASET_ROOT, f) for f in os.listdir(DATASET_ROOT)\n",
        "    if f.startswith('transforms_') and f.endswith('.json')\n",
        "]\n",
        "\n",
        "print(f\"\\nFichiers transforms trouv√©s:\")\n",
        "for tf in transforms_files:\n",
        "    print(f\"  ‚Ä¢ {os.path.basename(tf)}\")\n",
        "\n",
        "# V√©rifier les chemins dans les fichiers transforms\n",
        "if transforms_files:\n",
        "    import json\n",
        "    with open(transforms_files[0], 'r') as f:\n",
        "        data = json.load(f)\n",
        "        if 'frames' in data and len(data['frames']) > 0:\n",
        "            print(f\"\\nPremier chemin d'image trouv√©: {data['frames'][0].get('file_path', 'Non sp√©cifi√©')}\")\n",
        "\n",
        "# ==========================================\n",
        "# CR√âER UN DOSSIER TEMPORAIRE AVEC LA STRUCTURE ATTENDUE PAR BARF\n",
        "# ==========================================\n",
        "print(\"\\nüîÑ Cr√©ation structure temporaire pour BARF...\")\n",
        "\n",
        "# Cr√©er un dossier temporaire avec la structure attendue\n",
        "TEMP_DATASET_ROOT = '/tmp/barf_dataset'\n",
        "temp_scene_dir = os.path.join(TEMP_DATASET_ROOT, 'vision_scene')\n",
        "os.makedirs(temp_scene_dir, exist_ok=True)\n",
        "\n",
        "# Copier les fichiers transforms\n",
        "for tf in transforms_files:\n",
        "    import shutil\n",
        "    dest = os.path.join(temp_scene_dir, os.path.basename(tf))\n",
        "    shutil.copy2(tf, dest)\n",
        "    print(f\"  ‚Ä¢ Copi√© {os.path.basename(tf)} -> {dest}\")\n",
        "\n",
        "# V√©rifier et corriger les chemins dans les fichiers transforms\n",
        "for tf_name in ['transforms_train.json', 'transforms_test.json', 'transforms_val.json']:\n",
        "    tf_path = os.path.join(temp_scene_dir, tf_name)\n",
        "    if os.path.exists(tf_path):\n",
        "        with open(tf_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Corriger les chemins pour qu'ils pointent vers le bon dossier\n",
        "        frames_updated = False\n",
        "        for frame in data.get('frames', []):\n",
        "            if 'file_path' in frame:\n",
        "                original_path = frame['file_path']\n",
        "                # Si le chemin commence par \"./images/\", le corriger\n",
        "                if original_path.startswith('./images/'):\n",
        "                    # Garder juste le nom du fichier sans extension\n",
        "                    img_name = os.path.basename(original_path)\n",
        "                    # BARF s'attend √† des chemins relatifs au dossier de la sc√®ne\n",
        "                    frame['file_path'] = f\"./images/{img_name}\"\n",
        "                    frames_updated = True\n",
        "\n",
        "        if frames_updated:\n",
        "            with open(tf_path, 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "            print(f\"  ‚Ä¢ Chemins corrig√©s dans {tf_name}\")\n",
        "\n",
        "# Copier les images si n√©cessaire (cr√©er un lien symbolique)\n",
        "images_src = os.path.join(DATASET_ROOT, 'images')\n",
        "images_dest = os.path.join(temp_scene_dir, 'images')\n",
        "if os.path.exists(images_src) and not os.path.exists(images_dest):\n",
        "    os.symlink(images_src, images_dest)\n",
        "    print(f\"  ‚Ä¢ Lien symbolique cr√©√©: {images_src} -> {images_dest}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Structure temporaire cr√©√©e: {temp_scene_dir}\")\n",
        "\n",
        "# ==========================================\n",
        "# YAML CONFIG\n",
        "# ==========================================\n",
        "yaml_path = os.path.join(BARF_PATH, 'options', 'barf_blender_eval.yaml')\n",
        "yaml_orig = os.path.join(BARF_PATH, 'options', 'barf_blender.yaml')\n",
        "\n",
        "with open(yaml_orig, 'r') as f:\n",
        "    yaml = f.read()\n",
        "yaml = re.sub(r'visdom:\\s*(false|False)', 'visdom:\\n  cam_depth: 0.5', yaml)\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml)\n",
        "\n",
        "# ==========================================\n",
        "# LISTE DES CHECKPOINTS\n",
        "# ==========================================\n",
        "ckpts = sorted([f for f in os.listdir(CHECKPOINTS_PATH) if f.endswith('.ckpt')])\n",
        "\n",
        "print(f\"\\nüìÇ {len(ckpts)} checkpoints trouv√©s dans {CHECKPOINTS_PATH}:\")\n",
        "for c in ckpts[:10]:  # Afficher les 10 premiers\n",
        "    print(f\"   ‚Ä¢ {c}\")\n",
        "if len(ckpts) > 10:\n",
        "    print(f\"   ... et {len(ckpts)-10} autres\")\n",
        "\n",
        "# ==========================================\n",
        "# √âVALUATION DE TOUS LES CHECKPOINTS\n",
        "# ==========================================\n",
        "all_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ D√âBUT DES √âVALUATIONS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for idx, ckpt in enumerate(ckpts, 1):\n",
        "    ckpt_path = os.path.join(CHECKPOINTS_PATH, ckpt)\n",
        "    iteration = ckpt.replace('.ckpt', '').replace('model_', '')\n",
        "\n",
        "    print(f\"[{idx}/{len(ckpts)}] √âvaluation de {ckpt}...\")\n",
        "\n",
        "    # Commande avec la structure temporaire\n",
        "    cmd = [\n",
        "        \"python\", os.path.join(BARF_PATH, \"evaluate.py\"),\n",
        "        f\"--group=barf_eval_{iteration}\",\n",
        "        \"--model=barf\",\n",
        "        \"--yaml=barf_blender_eval\",\n",
        "        \"--data.scene=vision_scene\",\n",
        "        f\"--data.root={TEMP_DATASET_ROOT}\",\n",
        "        f\"--load={ckpt_path}\",\n",
        "        \"--data.val_on_test=true\",\n",
        "        \"--data.dataset=blender\"\n",
        "    ]\n",
        "\n",
        "    success = False\n",
        "    metrics = {'checkpoint': ckpt, 'iteration': iteration}\n",
        "\n",
        "    try:\n",
        "        print(f\"   Ex√©cution de la commande...\")\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, cwd=BARF_PATH, timeout=600)\n",
        "        output = result.stdout + result.stderr\n",
        "\n",
        "        # Afficher les premi√®res et derni√®res lignes pour debug\n",
        "        output_lines = output.split('\\n')\n",
        "        print(f\"   Return code: {result.returncode}\")\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"   ‚ö†Ô∏è Erreurs d√©tect√©es:\")\n",
        "            for line in output_lines[-20:]:  # Afficher les 20 derni√®res lignes\n",
        "                if line.strip():\n",
        "                    print(f\"     {line[:100]}\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Commande ex√©cut√©e avec succ√®s\")\n",
        "\n",
        "            # Sauvegarder le log complet\n",
        "            log_file = os.path.join(RESULTS_PATH, f\"log_{ckpt.replace('.ckpt', '')}.txt\")\n",
        "            with open(log_file, 'w') as f:\n",
        "                f.write(output)\n",
        "\n",
        "        # Extraction m√©triques - chercher le format standard BARF\n",
        "        for line in output_lines:\n",
        "            line_lower = line.lower()\n",
        "\n",
        "            # Format: \"PSNR:  24.56\" dans les dashboards\n",
        "            if 'psnr:' in line_lower and not 'psnr_fine' in line_lower:\n",
        "                matches = re.findall(r'([\\d.]+)', line)\n",
        "                if matches:\n",
        "                    try:\n",
        "                        metrics['PSNR'] = float(matches[-1])\n",
        "                        success = True\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Format: \"SSIM:  0.8765\"\n",
        "            elif 'ssim:' in line_lower:\n",
        "                matches = re.findall(r'([\\d.]+)', line)\n",
        "                if matches:\n",
        "                    try:\n",
        "                        metrics['SSIM'] = float(matches[-1])\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Format: \"LPIPS: 0.1234\"\n",
        "            elif 'lpips:' in line_lower:\n",
        "                matches = re.findall(r'([\\d.]+)', line)\n",
        "                if matches:\n",
        "                    try:\n",
        "                        metrics['LPIPS'] = float(matches[-1])\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        # Chercher aussi dans le format de sortie terminal\n",
        "        for i, line in enumerate(output_lines):\n",
        "            if '--------------------------' in line:\n",
        "                # Regarder les 10 lignes apr√®s chaque s√©parateur\n",
        "                for j in range(i+1, min(i+11, len(output_lines))):\n",
        "                    content_line = output_lines[j]\n",
        "                    if 'PSNR:' in content_line and 'PSNR' not in metrics:\n",
        "                        matches = re.findall(r'([\\d.]+)', content_line)\n",
        "                        if matches:\n",
        "                            try:\n",
        "                                metrics['PSNR'] = float(matches[0])\n",
        "                                success = True\n",
        "                            except:\n",
        "                                pass\n",
        "                    elif 'SSIM:' in content_line and 'SSIM' not in metrics:\n",
        "                        matches = re.findall(r'([\\d.]+)', content_line)\n",
        "                        if matches:\n",
        "                            try:\n",
        "                                metrics['SSIM'] = float(matches[0])\n",
        "                            except:\n",
        "                                pass\n",
        "                    elif 'LPIPS:' in content_line and 'LPIPS' not in metrics:\n",
        "                        matches = re.findall(r'([\\d.]+)', content_line)\n",
        "                        if matches:\n",
        "                            try:\n",
        "                                metrics['LPIPS'] = float(matches[0])\n",
        "                            except:\n",
        "                                pass\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"   ‚ùå Timeout (10 minutes)\")\n",
        "        metrics['PSNR'] = None\n",
        "        metrics['SSIM'] = None\n",
        "        metrics['LPIPS'] = None\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Erreur d'ex√©cution: {str(e)}\")\n",
        "        metrics['PSNR'] = None\n",
        "        metrics['SSIM'] = None\n",
        "        metrics['LPIPS'] = None\n",
        "\n",
        "    # V√©rifier si √©valuation a r√©ussi\n",
        "    if success and 'PSNR' in metrics:\n",
        "        print(f\"   ‚úÖ PSNR: {metrics['PSNR']:.2f} dB | SSIM: {metrics.get('SSIM', 0):.4f} | LPIPS: {metrics.get('LPIPS', 0):.4f}\")\n",
        "        all_results.append(metrics)\n",
        "    else:\n",
        "        print(f\"   ‚ùå √âchec - m√©triques non trouv√©es\")\n",
        "        if 'PSNR' not in metrics:\n",
        "            metrics['PSNR'] = None\n",
        "        if 'SSIM' not in metrics:\n",
        "            metrics['SSIM'] = None\n",
        "        if 'LPIPS' not in metrics:\n",
        "            metrics['LPIPS'] = None\n",
        "        all_results.append(metrics)\n",
        "\n",
        "        # Afficher la commande pour debug manuel\n",
        "        print(f\"   üí° Commande ex√©cut√©e:\")\n",
        "        print(f\"   {' '.join(cmd)}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "# ==========================================\n",
        "# R√âSUM√â GLOBAL\n",
        "# ==========================================\n",
        "print(\"=\"*70)\n",
        "print(\"üìä R√âSUM√â GLOBAL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all_results:\n",
        "    df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Trier par iteration number\n",
        "    try:\n",
        "        df['iter_num'] = pd.to_numeric(df['iteration'], errors='coerce')\n",
        "        df = df.sort_values('iter_num')\n",
        "        df = df.drop('iter_num', axis=1)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"{'Checkpoint':<25} {'Iter':<8} {'PSNR (dB)':<12} {'SSIM':<10} {'LPIPS':<10}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        psnr = f\"{row['PSNR']:.2f}\" if pd.notna(row['PSNR']) else \"N/A\"\n",
        "        ssim = f\"{row['SSIM']:.4f}\" if pd.notna(row['SSIM']) else \"N/A\"\n",
        "        lpips = f\"{row['LPIPS']:.4f}\" if pd.notna(row['LPIPS']) else \"N/A\"\n",
        "        checkpoint_short = row['checkpoint'][:20] + '...' if len(row['checkpoint']) > 23 else row['checkpoint']\n",
        "        print(f\"{checkpoint_short:<25} {row['iteration']:<8} {psnr:<12} {ssim:<10} {lpips:<10}\")\n",
        "\n",
        "    # Statistiques\n",
        "    valid_results = df[df['PSNR'].notna()].copy()\n",
        "\n",
        "    if len(valid_results) > 0:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìà STATISTIQUES\")\n",
        "        print(\"-\"*70)\n",
        "        print(f\"Checkpoints √©valu√©s : {len(valid_results)}/{len(ckpts)}\")\n",
        "\n",
        "        if not valid_results.empty:\n",
        "            # Meilleur PSNR\n",
        "            best_psnr_idx = valid_results['PSNR'].idxmax()\n",
        "            print(f\"\\nMeilleur PSNR       : {valid_results.loc[best_psnr_idx, 'PSNR']:.2f} dB ({valid_results.loc[best_psnr_idx, 'checkpoint']})\")\n",
        "\n",
        "            # Meilleur SSIM\n",
        "            if 'SSIM' in valid_results.columns and valid_results['SSIM'].notna().any():\n",
        "                best_ssim_idx = valid_results['SSIM'].idxmax()\n",
        "                print(f\"Meilleur SSIM       : {valid_results.loc[best_ssim_idx, 'SSIM']:.4f} ({valid_results.loc[best_ssim_idx, 'checkpoint']})\")\n",
        "\n",
        "            # Meilleur LPIPS\n",
        "            if 'LPIPS' in valid_results.columns and valid_results['LPIPS'].notna().any():\n",
        "                best_lpips_idx = valid_results['LPIPS'].idxmin()\n",
        "                print(f\"Meilleur LPIPS      : {valid_results.loc[best_lpips_idx, 'LPIPS']:.4f} ({valid_results.loc[best_lpips_idx, 'checkpoint']})\")\n",
        "\n",
        "            print(f\"\\nMoyenne PSNR        : {valid_results['PSNR'].mean():.2f} dB\")\n",
        "            if 'SSIM' in valid_results.columns and valid_results['SSIM'].notna().any():\n",
        "                print(f\"Moyenne SSIM        : {valid_results['SSIM'].mean():.4f}\")\n",
        "            if 'LPIPS' in valid_results.columns and valid_results['LPIPS'].notna().any():\n",
        "                print(f\"Moyenne LPIPS       : {valid_results['LPIPS'].mean():.4f}\")\n",
        "\n",
        "    # Sauvegarder CSV\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    csv_path = os.path.join(RESULTS_PATH, f\"all_checkpoints_{timestamp}.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüíæ CSV sauvegard√©: {csv_path}\")\n",
        "\n",
        "    # Graphique √©volution (texte ASCII)\n",
        "    if len(valid_results) > 1 and 'PSNR' in valid_results.columns:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìà √âVOLUTION DU PSNR\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        psnr_values = valid_results['PSNR'].values\n",
        "        min_psnr = psnr_values.min()\n",
        "        max_psnr = psnr_values.max()\n",
        "\n",
        "        for _, row in valid_results.iterrows():\n",
        "            psnr = row['PSNR']\n",
        "            # Normaliser entre 0 et 50 caract√®res\n",
        "            if pd.notna(psnr) and max_psnr > min_psnr:\n",
        "                bar_len = int(((psnr - min_psnr) / (max_psnr - min_psnr)) * 50)\n",
        "            else:\n",
        "                bar_len = 25\n",
        "            bar = '‚ñà' * bar_len\n",
        "            checkpoint_short = row['checkpoint'][:12] + '...' if len(row['checkpoint']) > 15 else row['checkpoint']\n",
        "            print(f\"{checkpoint_short:<15} {bar} {psnr:.2f} dB\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå Aucun r√©sultat obtenu\")\n",
        "\n",
        "# Rapport d√©taill√©\n",
        "report_path = os.path.join(RESULTS_PATH, f\"rapport_complet_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(\"=\"*70 + \"\\n\")\n",
        "    f.write(\"RAPPORT D'√âVALUATION COMPL√àTE - TOUS LES CHECKPOINTS\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Nombre de checkpoints: {len(ckpts)}\\n\")\n",
        "    f.write(f\"Checkpoints path: {CHECKPOINTS_PATH}\\n\")\n",
        "    f.write(f\"Dataset original: {DATASET_ROOT}\\n\")\n",
        "    f.write(f\"Dataset temporaire: {TEMP_DATASET_ROOT}\\n\")\n",
        "    f.write(f\"Scene name: vision_scene\\n\\n\")\n",
        "\n",
        "    f.write(\"STRUCTURE DATASET:\\n\")\n",
        "    f.write(\"-\"*70 + \"\\n\")\n",
        "    f.write(f\"Transforms files:\\n\")\n",
        "    for tf in transforms_files:\n",
        "        f.write(f\"  ‚Ä¢ {os.path.basename(tf)}\\n\")\n",
        "\n",
        "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "    f.write(\"R√âSULTATS D√âTAILL√âS:\\n\")\n",
        "    f.write(\"-\"*70 + \"\\n\")\n",
        "    if all_results:\n",
        "        for r in all_results:\n",
        "            f.write(f\"\\nCheckpoint: {r['checkpoint']}\\n\")\n",
        "            f.write(f\"Iteration: {r['iteration']}\\n\")\n",
        "            f.write(f\"  PSNR  : {r.get('PSNR', 'N/A')}\\n\")\n",
        "            f.write(f\"  SSIM  : {r.get('SSIM', 'N/A')}\\n\")\n",
        "            f.write(f\"  LPIPS : {r.get('LPIPS', 'N/A')}\\n\")\n",
        "    else:\n",
        "        f.write(\"\\nAucun r√©sultat disponible\\n\")\n",
        "\n",
        "print(f\"\\nüíæ Rapport d√©taill√©: {report_path}\")\n",
        "\n",
        "# Nettoyage (optionnel)\n",
        "import shutil\n",
        "if os.path.exists(TEMP_DATASET_ROOT):\n",
        "    shutil.rmtree(TEMP_DATASET_ROOT)\n",
        "    print(f\"üßπ Structure temporaire nettoy√©e: {TEMP_DATASET_ROOT}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ √âVALUATION COMPL√àTE TERMIN√âE\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
